{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "import nltk \n",
    "from nltk.tokenize import word_tokenize\n",
    "import datetime\n",
    "import time\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pickle\n",
    "from numpy.linalg import cholesky\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newsample(nnn,ratio):\n",
    "    if ratio >len(nnn):\n",
    "        return random.sample(nnn*(ratio//len(nnn)+1),ratio)\n",
    "    else:\n",
    "        return random.sample(nnn,ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_user_file(file='ClickData4.tsv',npratio=4):\n",
    "    userid_dict={}\n",
    "    with open(file) as f:\n",
    "        userdata=f.readlines()\n",
    "    for user in userdata:\n",
    "        line=user.strip().split('\\t')\n",
    "        userid=line[0]\n",
    "        # map user_id to index\n",
    "        if userid not in userid_dict:\n",
    "            userid_dict[userid]=len(userid_dict)\n",
    "    \n",
    "    all_train_id=[]\n",
    "    all_train_pn=[]    \n",
    "    all_label=[]\n",
    "    \n",
    "    all_test_id=[]\n",
    "    all_test_pn=[]    \n",
    "    all_test_label=[]\n",
    "    all_test_index=[]\n",
    "    \n",
    "    all_user_pos=[]\n",
    "    all_test_user_pos=[]\n",
    "    \n",
    "    for user in userdata:\n",
    "        line=user.strip().split('\\t')\n",
    "        userid=line[0]\n",
    "        # split userdata in impressions\n",
    "        if len(line)==4:\n",
    "            impre=[x.split('#TAB#') for x in line[2].split('#N#')]\n",
    "        if len(line)==3:\n",
    "            impre=[x.split('#TAB#') for x in line[2].split('#N#')]\n",
    "        else:\n",
    "            raise NotImplementedError() # HB edit\n",
    "    \n",
    "        trainpos=[x[0].split() for x in impre]\n",
    "        trainneg=[x[1].split() for x in impre]\n",
    "         \n",
    "        poslist=list(itertools.chain(*(trainpos)))\n",
    "        neglist=list(itertools.chain(*(trainneg)))\n",
    "    \n",
    "        \n",
    "        if len(line)==4:\n",
    "            testimpre=[x.split('#TAB#') for x in line[3].split('#N#')]\n",
    "            testpos=[x[0].split() for x in testimpre]\n",
    "            testneg=[x[1].split() for x in testimpre]\n",
    "            \n",
    "            \n",
    "            for i in range(len(testpos)):\n",
    "                sess_index=[]\n",
    "                sess_index.append(len(all_test_pn))\n",
    "                posset=list(set(poslist))\n",
    "                allpos=[int(p) for p in random.sample(posset,min(50,len(posset)))[:50]]\n",
    "                allpos+=[0]*(50-len(allpos))\n",
    "        \n",
    "                \n",
    "                for j in testpos[i]:\n",
    "                    all_test_pn.append(int(j))\n",
    "                    all_test_label.append(1)\n",
    "                    all_test_id.append(userid_dict[userid])\n",
    "                    all_test_user_pos.append(allpos)\n",
    "                    \n",
    "                for j in testneg[i]:\n",
    "                    all_test_pn.append(int(j))\n",
    "                    all_test_label.append(0)\n",
    "                    all_test_id.append(userid_dict[userid])\n",
    "                    all_test_user_pos.append(allpos)\n",
    "                sess_index.append(len(all_test_pn))\n",
    "                all_test_index.append(sess_index)\n",
    "                \n",
    "    \n",
    "    \n",
    "                \n",
    "        for impre_id in range(len(trainpos)):\n",
    "            for pos_sample in trainpos[impre_id]:\n",
    "    \n",
    "                pos_neg_sample=newsample(trainneg[impre_id],npratio)\n",
    "                pos_neg_sample.append(pos_sample)\n",
    "                temp_label=[0]*npratio+[1]\n",
    "                temp_id=list(range(npratio+1))\n",
    "                random.shuffle(temp_id)\n",
    "    \n",
    "                \n",
    "                shuffle_sample=[]\n",
    "                shuffle_label=[]\n",
    "                for id in temp_id:\n",
    "                    shuffle_sample.append(int(pos_neg_sample[id]))\n",
    "                    shuffle_label.append(temp_label[id])\n",
    "                \n",
    "                posset=list(set(poslist)-set([pos_sample]))\n",
    "                allpos=[int(p) for p in random.sample(posset,min(50,len(posset)))[:50]]\n",
    "                allpos+=[0]*(50-len(allpos))\n",
    "                all_train_pn.append(shuffle_sample)\n",
    "                all_label.append(shuffle_label)\n",
    "                all_train_id.append(userid_dict[userid])\n",
    "                all_user_pos.append(allpos)\n",
    "            \n",
    "    all_train_pn=np.array(all_train_pn,dtype='int32')\n",
    "    all_label=np.array(all_label,dtype='int32')\n",
    "    all_train_id=np.array(all_train_id,dtype='int32')\n",
    "    all_test_pn=np.array(all_test_pn,dtype='int32')\n",
    "    all_test_label=np.array(all_test_label,dtype='int32')\n",
    "    all_test_id=np.array(all_test_id,dtype='int32')\n",
    "    all_user_pos=np.array(all_user_pos,dtype='int32')\n",
    "    all_test_user_pos=np.array(all_test_user_pos,dtype='int32')\n",
    "    return userid_dict,all_train_pn,all_label,all_train_id,all_test_pn,all_test_label,all_test_id,all_user_pos,all_test_user_pos,all_test_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = True\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_news_file(file='DocMeta3.tsv'):\n",
    "    with open(file) as f:\n",
    "        newsdata=f.readlines()\n",
    "    \n",
    "    news={}\n",
    "    for newsline in newsdata:\n",
    "        line=newsline.strip().split('\\t')\n",
    "        news[line[1]]=[line[2],line[3],word_tokenize(line[6].lower())]\n",
    "    word_dict_raw={'PADDING':[0,999999]} # key: 'word', value: [index, counts]\n",
    "    \n",
    "    for docid in news:\n",
    "        for word in news[docid][2]:\n",
    "            if word in word_dict_raw:\n",
    "                word_dict_raw[word][1]+=1\n",
    "            else:\n",
    "                word_dict_raw[word]=[len(word_dict_raw),1]\n",
    "    word_dict={}\n",
    "    for i in word_dict_raw:\n",
    "        # only include word with counter > 2\n",
    "        if word_dict_raw[i][1]>=2:\n",
    "            word_dict[i]=[len(word_dict),word_dict_raw[i][1]]\n",
    "    print(\"Vocab: {}  Raw: {}\".format(len(word_dict),len(word_dict_raw)))\n",
    "    \n",
    "    news_words=[[0]*30] # max_len_news_title = 30\n",
    "    news_index={'0':0} # dictionary news indices\n",
    "    for newsid in news:\n",
    "        word_id=[]\n",
    "        news_index[newsid]=len(news_index)\n",
    "        # get word_ids from news title\n",
    "        for word in news[newsid][2]:\n",
    "            if word in word_dict:\n",
    "                word_id.append(word_dict[word][0])\n",
    "        \n",
    "        #pad sequence\n",
    "        word_id=word_id[:30] # max_len_news_title\n",
    "        news_words.append(word_id+[0]*(30-len(word_id))) # note: 0 as padding value\n",
    "        \n",
    "    news_words=np.array(news_words,dtype='int32') \n",
    "    return word_dict,news_words,news_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(word_dict):\n",
    "    embedding_dict={}\n",
    "    \n",
    "    # for each word in vocabulary, look up and store glove embedding vector\n",
    "    # words from dictionary that don't appear in Glove are discarded (?)\n",
    "    cnt=0\n",
    "    with open('/data/wuch/glove.840B.300d.txt','rb')as f:\n",
    "        linenb=0\n",
    "        while True:\n",
    "            line=f.readline()\n",
    "            if len(line)==0:\n",
    "                break\n",
    "            line = line.split()\n",
    "            word=line[0].decode()\n",
    "            linenb+=1\n",
    "            if len(word) != 0:\n",
    "                vec=[float(x) for x in line[1:]]\n",
    "                # only store words that appear in dictionary\n",
    "                if word in word_dict:\n",
    "                    embedding_dict[word]=vec\n",
    "                    if cnt%1000==0:\n",
    "                        print(cnt,linenb,word)\n",
    "                    cnt+=1\n",
    "    \n",
    "    embedding_matrix=[0]*len(word_dict)\n",
    "    cand=[]\n",
    "    # convert embedding dict into matrix\n",
    "    # pretrained values are floats\n",
    "    # \n",
    "    for i in embedding_dict:\n",
    "        embedding_matrix[word_dict[i][0]]=np.array(embedding_dict[i],dtype='float32')\n",
    "        cand.append(embedding_matrix[word_dict[i][0]])\n",
    "    cand=np.array(cand,dtype='float32')\n",
    "    mu=np.mean(cand, axis=0)\n",
    "    Sigma=np.cov(cand.T)\n",
    "    norm=np.random.multivariate_normal(mu, Sigma, 1)\n",
    "    for i in range(len(embedding_matrix)):\n",
    "        # if embedding is int, i.e. no pretrained value, initialise with values from normal distribution\n",
    "        if type(embedding_matrix[i])==int:\n",
    "            embedding_matrix[i]=np.reshape(norm, 300)\n",
    "    embedding_matrix[0]=np.zeros(300,dtype='float32') # placeholder with zero values (?)\n",
    "    embedding_matrix=np.array(embedding_matrix,dtype='float32')\n",
    "    print(embedding_matrix.shape)\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "userid_dict, all_train_pn, all_label, all_train_id, \\\n",
    "    all_test_pn, all_test_label, all_test_id, \\\n",
    "    all_user_pos, all_test_user_pos, all_test_index \\\n",
    "        = preprocess_user_file(file='./ClickData_sample.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(userid_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1291"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_train_pn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10501, 29467, 13735, 31165, 26836],\n",
       "       [22212, 13735, 15719, 36713,  4450],\n",
       "       [38400, 39980, 27040, 17229, 18817],\n",
       "       [12238, 28796,  2083, 21200, 30403]], dtype=int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_train_pn[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 1],\n",
       "       [1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 1, 0]], dtype=int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_label[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_train_id[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1291"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_train_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7363"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_test_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56 388\n"
     ]
    }
   ],
   "source": [
    "word_dict, news_words, news_index = preprocess_news_file(file='./DocMeta_sample.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('PADDING', [0, 999999]), ('the', [1, 10]), ('2018', [2, 4]), ('?', [3, 2]), ('are', [4, 3]), ('15', [5, 2]), ('for', [6, 12]), ('13', [7, 3]), ('tips', [8, 2]), ('that', [9, 6]), ('can', [10, 2]), ('help', [11, 2]), ('you', [12, 4]), ('your', [13, 3]), ('at', [14, 4]), ('dainese', [15, 2]), (\"'s\", [16, 9]), ('vision', [17, 2]), ('of', [18, 10]), ('a', [19, 6]), ('and', [20, 7]), ('best', [21, 5]), ('in', [22, 6]), ('work', [23, 3]), ('will', [24, 2]), ('return', [25, 2]), (',', [26, 10]), ('with', [27, 7]), (':', [28, 5]), (\"'\", [29, 6]), ('top', [30, 2]), ('games', [31, 3]), ('have', [32, 3]), ('all', [33, 3]), ('over', [34, 3]), ('get', [35, 2]), ('from', [36, 3]), ('out', [37, 2]), ('on', [38, 3]), ('could', [39, 2]), ('new', [40, 2]), ('tom', [41, 2]), ('is', [42, 3]), ('list', [43, 2]), ('to', [44, 11]), ('priest', [45, 2]), ('how', [46, 2]), ('his', [47, 2]), ('says', [48, 2]), ('$', [49, 2]), ('stores', [50, 2]), ('cities', [51, 2]), ('1', [52, 2]), ('go', [53, 2]), ('hour', [54, 2]), ('2019', [55, 3])])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dict.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51, 30)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_words.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/data/wuch/glove.840B.300d.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-09d8831bc382>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0membedding_mat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-50895c503bb1>\u001b[0m in \u001b[0;36mget_embedding\u001b[0;34m(word_dict)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0membedding_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mcnt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/data/wuch/glove.840B.300d.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mlinenb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/data/wuch/glove.840B.300d.txt'"
     ]
    }
   ],
   "source": [
    "embedding_mat=get_embedding(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def dcg_score(y_true, y_score, k=10):\n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    y_true = np.take(y_true, order[:k])\n",
    "    gains = 2 ** y_true - 1\n",
    "    discounts = np.log2(np.arange(len(y_true)) + 2)\n",
    "    return np.sum(gains / discounts)\n",
    "\n",
    "\n",
    "def ndcg_score(y_true, y_score, k=10):\n",
    "    best = dcg_score(y_true, y_true, k)\n",
    "    actual = dcg_score(y_true, y_score, k)\n",
    "    return actual / best\n",
    "\n",
    "\n",
    "def mrr_score(y_true, y_score):\n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    y_true = np.take(y_true, order)\n",
    "    rr_score = y_true / (np.arange(len(y_true)) + 1)\n",
    "    return np.sum(rr_score) / np.sum(y_true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_batch_data_train(all_train_pn,all_label,all_train_id,batch_size):\n",
    "    inputid = np.arange(len(all_label))\n",
    "    np.random.shuffle(inputid)\n",
    "    y=all_label\n",
    "    batches = [inputid[range(batch_size*i, min(len(y), batch_size*(i+1)))] for i in range(len(y)//batch_size+1)]\n",
    "\n",
    "    while (True):\n",
    "        for i in batches:\n",
    "            candidate = news_words[all_train_pn[i]]\n",
    "            candidate_split=[candidate[:,k,:] for k in range(candidate.shape[1])]\n",
    "            browsed_news=news_words[all_user_pos[i]]\n",
    "            browsed_news_split=[browsed_news[:,k,:] for k in range(browsed_news.shape[1])]\n",
    "            userid=np.expand_dims(all_train_id[i],axis=1)\n",
    "            label=all_label[i]\n",
    "            yield (candidate_split +browsed_news_split+[userid], label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch_data_test(all_test_pn,all_label,all_test_id,batch_size):\n",
    "    inputid = np.arange(len(all_label))\n",
    "    y=all_label\n",
    "    batches = [inputid[range(batch_size*i, min(len(y), batch_size*(i+1)))] for i in range(len(y)//batch_size+1)]\n",
    "\n",
    "    while (True):\n",
    "        for i in batches:\n",
    "            candidate = news_words[all_test_pn[i]]\n",
    "            browsed_news=news_words[all_test_user_pos[i]]\n",
    "            browsed_news_split=[browsed_news[:,k,:] for k in range(browsed_news.shape[1])]\n",
    "            userid=np.expand_dims(all_test_id[i],axis=1)\n",
    "            label=all_label[i]\n",
    "\n",
    "            yield ([candidate]+ browsed_news_split+[userid], label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "import keras\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.optimizers import *\n",
    "npratio=4\n",
    "results=[]\n",
    "    \n",
    "    \n",
    "MAX_SENT_LENGTH=30\n",
    "MAX_SENTS=50\n",
    "\n",
    "##user embedding - word & article level\n",
    "user_id = Input(shape=(1,), dtype='int32')\n",
    "user_embedding_layer= Embedding(len(userid_dict), 50,trainable=True)\n",
    "user_embedding= user_embedding_layer(user_id)\n",
    "user_embedding_word= Dense(200,activation='relu')(user_embedding)\n",
    "user_embedding_word= Flatten()(user_embedding_word)\n",
    "user_embedding_news= Dense(200,activation='relu')(user_embedding)\n",
    "user_embedding_news= Flatten()(user_embedding_news)\n",
    "\n",
    "##news encoder\n",
    "news_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n",
    "embedding_layer = Embedding(len(word_dict) , 300, weights=[embedding_mat],trainable=True)\n",
    "embedded_sequences = embedding_layer(news_input)\n",
    "embedded_sequences =Dropout(0.2)(embedded_sequences)\n",
    "\n",
    "cnnouput = Convolution1D(nb_filter=400, filter_length=3,  padding='same', activation='relu', strides=1)(embedded_sequences )\n",
    "cnnouput=Dropout(0.2)(cnnouput)\n",
    "\n",
    "    # personalised attention - word level\n",
    "attention_a = Dot((2, 1))([cnnouput, Dense(400,activation='tanh')(user_embedding_word)])\n",
    "attention_weight = Activation('softmax')(attention_a)\n",
    "news_rep=keras.layers.Dot((1, 1))([cnnouput, attention_weight])\n",
    "newsEncoder = Model([news_input,user_id], news_rep)\n",
    "\n",
    "# browsing history as concatenation of MAX_SENTS articles\n",
    "all_news_input = [keras.Input((MAX_SENT_LENGTH,), dtype='int32') for _ in range(MAX_SENTS)]\n",
    "browsed_news_rep = [newsEncoder([news,user_id]) for news in all_news_input]\n",
    "browsed_news_rep =concatenate([Lambda(lambda x: K.expand_dims(x,axis=1))(news) for news in browsed_news_rep],axis=1)\n",
    "\n",
    "## user encoder\n",
    "# personalised attention - article level\n",
    "attention_news = keras.layers.Dot((2, 1))([browsed_news_rep, Dense(400,activation='tanh')(user_embedding_news)])\n",
    "attention_weight_news = Activation('softmax')(attention_news)\n",
    "user_rep=keras.layers.Dot((1, 1))([browsed_news_rep, attention_weight_news])\n",
    "\n",
    "# candidate items - as pseudo K + 1 classification task\n",
    "candidates = [keras.Input((MAX_SENT_LENGTH,), dtype='int32') for _ in range(1+npratio)]\n",
    "candidate_vecs = [ newsEncoder([candidate,user_id]) for candidate in candidates]\n",
    "logits = [keras.layers.dot([user_rep, candidate_vec], axes=-1) for candidate_vec in candidate_vecs]\n",
    "logits = keras.layers.Activation(keras.activations.softmax)(keras.layers.concatenate(logits))\n",
    "\n",
    "model = Model(candidates+all_news_input+[user_id], logits)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001), metrics=['acc'])\n",
    "\n",
    "\n",
    "candidate_one = keras.Input((MAX_SENT_LENGTH,))\n",
    "candidate_one_vec = newsEncoder([candidate_one,user_id])\n",
    "score = keras.layers.Activation(keras.activations.sigmoid)(keras.layers.dot([user_rep, candidate_one_vec], axes=-1))\n",
    "model_test = keras.Model([candidate_one]+all_news_input+[user_id], score)\n",
    "\n",
    "for ep in range(3):\n",
    "    traingen=generate_batch_data_train(all_train_pn,all_label,all_train_id, 100)\n",
    "    model.fit_generator(traingen, epochs=1,steps_per_epoch=len(all_train_id)//100)\n",
    "    testgen=generate_batch_data_test(all_test_pn,all_test_label,all_test_id, 100)\n",
    "    click_score = model_test.predict_generator(testgen, steps=len(all_test_id)//100,verbose=1)\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    all_auc=[]\n",
    "    all_mrr=[]\n",
    "    all_ndcg=[]\n",
    "    all_ndcg2=[]\n",
    "    for m in all_test_index:\n",
    "        if np.sum(all_test_label[m[0]:m[1]])!=0 and m[1]<len(click_score):\n",
    "            all_auc.append(roc_auc_score(all_test_label[m[0]:m[1]],click_score[m[0]:m[1],0]))\n",
    "            all_mrr.append(mrr_score(all_test_label[m[0]:m[1]],click_score[m[0]:m[1],0]))\n",
    "            all_ndcg.append(ndcg_score(all_test_label[m[0]:m[1]],click_score[m[0]:m[1],0],k=5))\n",
    "            all_ndcg2.append(ndcg_score(all_test_label[m[0]:m[1]],click_score[m[0]:m[1],0],k=10))\n",
    "    results.append([np.mean(all_auc),np.mean(all_mrr),np.mean(all_ndcg),np.mean(all_ndcg2)])\n",
    "    print(np.mean(all_auc),np.mean(all_mrr),np.mean(all_ndcg),np.mean(all_ndcg2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
